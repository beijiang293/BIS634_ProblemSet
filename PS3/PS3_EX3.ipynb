{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "## Gradient Descent for Optimizing Parameters `a` and `b`\n",
    "To implement the gradient descent algorithm for this problem, follow these steps:\n",
    "1. Query the given API to get the error value for a set of parameters `(a, b)`.\n",
    "2. Compute the gradient of the error with respect to both parameters.\n",
    "3. Update the parameters `a` and `b` in the direction that reduces the error.\n",
    "4. Repeat the above steps until the error converges to a minimum value or after a certain number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_error(a, b):\n",
    "    \"\"\"Query the API to get error for the given a and b values.\"\"\"\n",
    "    url = f\"http://ramcdougal.com/cgi-bin/error_function.py?a={a}&b={b}\"\n",
    "    return float(requests.get(url, headers={\"User-Agent\": \"MyScript\"}).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(a_start=0.5, b_start=0.5, learning_rate=0.1, iterations=100, delta=0.01):\n",
    "    \"\"\"Perform 2D gradient descent.\"\"\"\n",
    "    a = a_start\n",
    "    b = b_start\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        # Calculate gradient\n",
    "        error_current = get_error(a, b)\n",
    "        \n",
    "        a_gradient = (get_error(a + delta, b) - error_current) / delta\n",
    "        b_gradient = (get_error(a, b + delta) - error_current) / delta\n",
    "        \n",
    "        # Update a and b\n",
    "        a = a - learning_rate * a_gradient\n",
    "        b = b - learning_rate * b_gradient\n",
    "        \n",
    "        # Print the error for current iteration\n",
    "        print(f\"Iteration {i+1}: Error = {error_current}, a = {a}, b = {b}\")\n",
    "        \n",
    "    return a, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Error = 1.216377, a = 0.44220000000000104, b = 0.5368000000000013\n",
      "Iteration 2: Error = 1.17433128, a = 0.3959600000000014, b = 0.5662400000000019\n",
      "Iteration 3: Error = 1.1474556192, a = 0.3589680000000013, b = 0.5897920000000019\n",
      "Iteration 4: Error = 1.13028207629, a = 0.3293744000000016, b = 0.6086336000000032\n",
      "Iteration 5: Error = 1.11931251282, a = 0.3056995200000019, b = 0.6237068800000052\n",
      "Iteration 6: Error = 1.11230919541, a = 0.28675961600000033, b = 0.635765504000005\n",
      "Iteration 7: Error = 1.10784083482, a = 0.2716076927999995, b = 0.6454124032000048\n",
      "Iteration 8: Error = 1.10499209409, a = 0.2594861541999989, b = 0.6531299225000038\n",
      "Iteration 9: Error = 1.10317770807, a = 0.24978892340000014, b = 0.6593039380000039\n",
      "Iteration 10: Error = 1.10202354744, a = 0.24203113870000026, b = 0.664243150400003\n",
      "Iteration 11: Error = 1.10129052178, a = 0.23582491089999946, b = 0.6681945203000024\n",
      "Iteration 12: Error = 1.10082589508, a = 0.23085992869999972, b = 0.6713556163000041\n",
      "Iteration 13: Error = 1.10053214176, a = 0.22688794299999993, b = 0.673884493100005\n",
      "Iteration 14: Error = 1.10034702585, a = 0.22371035440000142, b = 0.6759075945000066\n",
      "Iteration 15: Error = 1.10023086065, a = 0.2211682836000013, b = 0.6775260756000074\n",
      "Iteration 16: Error = 1.1001583621, a = 0.21913462690000118, b = 0.6788208605000063\n",
      "Iteration 17: Error = 1.10011344077, a = 0.2175077016000011, b = 0.6798566884000063\n",
      "Iteration 18: Error = 1.10008587331, a = 0.2162061613000028, b = 0.6806853507000072\n",
      "Iteration 19: Error = 1.1000691759, a = 0.21516492910000418, b = 0.6813482806000088\n",
      "Iteration 20: Error = 1.10005924615, a = 0.21433194320000526, b = 0.6818786244000092\n",
      "Iteration 21: Error = 1.1000534964, a = 0.2136655545000039, b = 0.6823028995000087\n",
      "Iteration 22: Error = 1.10005030079, a = 0.21313244360000327, b = 0.6826423196000082\n",
      "Iteration 23: Error = 1.10004864298, a = 0.21270595490000277, b = 0.6829138557000087\n",
      "Iteration 24: Error = 1.10004789189, a = 0.21236476400000326, b = 0.68313108460001\n",
      "Iteration 25: Error = 1.10004765911, a = 0.2120918112000032, b = 0.6833048677000093\n",
      "Iteration 26: Error = 1.10004770847, a = 0.2118734489000036, b = 0.6834438941000083\n",
      "Iteration 27: Error = 1.10004789874, a = 0.21169875920000258, b = 0.683555115300007\n",
      "Iteration 28: Error = 1.10004814744, a = 0.21155900730000132, b = 0.683644092200006\n",
      "Iteration 29: Error = 1.10004840816, a = 0.2114472058000012, b = 0.6837152737000047\n",
      "Iteration 30: Error = 1.10004865627, a = 0.21135776470000156, b = 0.6837722190000046\n",
      "Iteration 31: Error = 1.10004888004, a = 0.21128621170000095, b = 0.6838177752000036\n",
      "Iteration 32: Error = 1.10004907525, a = 0.2112289693000009, b = 0.6838542201000037\n",
      "Iteration 33: Error = 1.10004924178, a = 0.2111831754000022, b = 0.6838833760000047\n",
      "Iteration 34: Error = 1.10004938164, a = 0.21114654030000413, b = 0.683906700800005\n",
      "Iteration 35: Error = 1.10004949777, a = 0.2111172323000039, b = 0.6839253607000035\n",
      "Iteration 36: Error = 1.10004959338, a = 0.2110937858000046, b = 0.6839402885000041\n",
      "Iteration 37: Error = 1.10004967162, a = 0.2110750287000025, b = 0.6839522308000041\n",
      "Iteration 38: Error = 1.10004973532, a = 0.21106002300000215, b = 0.6839617847000041\n",
      "Iteration 39: Error = 1.10004978699, a = 0.21104801840000142, b = 0.6839694278000037\n",
      "Iteration 40: Error = 1.10004982878, a = 0.21103841470000173, b = 0.6839755423000033\n",
      "Iteration 41: Error = 1.1000498625, a = 0.21103073170000108, b = 0.6839804338000017\n",
      "Iteration 42: Error = 1.10004988967, a = 0.211024585300001, b = 0.6839843470000013\n",
      "Iteration 43: Error = 1.10004991153, a = 0.21101966830000052, b = 0.6839874776000006\n",
      "Iteration 44: Error = 1.10004992908, a = 0.21101573460000145, b = 0.6839899820000013\n",
      "Iteration 45: Error = 1.10004994318, a = 0.21101258770000086, b = 0.6839919856000005\n",
      "Iteration 46: Error = 1.10004995449, a = 0.2110100702000013, b = 0.6839935884999999\n",
      "Iteration 47: Error = 1.10004996356, a = 0.21100805620000118, b = 0.6839948708000017\n",
      "Iteration 48: Error = 1.10004997082, a = 0.2110064449000011, b = 0.6839958966000022\n",
      "Iteration 49: Error = 1.10004997664, a = 0.21100515590000102, b = 0.6839967172000034\n",
      "Iteration 50: Error = 1.10004998131, a = 0.21100412480000008, b = 0.6839973738000023\n",
      "Iteration 51: Error = 1.10004998504, a = 0.21100329990000066, b = 0.6839978991000035\n",
      "Iteration 52: Error = 1.10004998803, a = 0.21100264000000157, b = 0.683998319300005\n",
      "Iteration 53: Error = 1.10004999042, a = 0.2110021120000023, b = 0.6839986555000062\n",
      "Iteration 54: Error = 1.10004999233, a = 0.21100168960000287, b = 0.6839989244000062\n",
      "Iteration 55: Error = 1.10004999386, a = 0.21100135160000155, b = 0.6839991395000062\n",
      "Iteration 56: Error = 1.10004999509, a = 0.2110010813000014, b = 0.6839993116000072\n",
      "Iteration 57: Error = 1.10004999607, a = 0.21100086500000126, b = 0.6839994493000074\n",
      "Iteration 58: Error = 1.10004999686, a = 0.21100069200000027, b = 0.6839995595000077\n",
      "Iteration 59: Error = 1.10004999749, a = 0.21100055359999992, b = 0.6839996476000083\n",
      "Iteration 60: Error = 1.10004999799, a = 0.21100044289999964, b = 0.6839997181000075\n",
      "Iteration 61: Error = 1.10004999839, a = 0.2110003543000012, b = 0.6839997745000077\n",
      "Iteration 62: Error = 1.10004999871, a = 0.211000283400002, b = 0.6839998196000092\n",
      "Iteration 63: Error = 1.10004999897, a = 0.21100022670000396, b = 0.68399985570001\n",
      "Iteration 64: Error = 1.10004999918, a = 0.21100018140000243, b = 0.6839998846000079\n",
      "Iteration 65: Error = 1.10004999934, a = 0.21100014510000165, b = 0.6839999077000076\n",
      "Iteration 66: Error = 1.10004999947, a = 0.21100011610000147, b = 0.6839999261000069\n",
      "Iteration 67: Error = 1.10004999958, a = 0.21100009290000177, b = 0.6839999409000059\n",
      "Iteration 68: Error = 1.10004999966, a = 0.21100007430000023, b = 0.6839999527000047\n",
      "Iteration 69: Error = 1.10004999973, a = 0.211000059399999, b = 0.6839999622000033\n",
      "Iteration 70: Error = 1.10004999978, a = 0.211000047499998, b = 0.6839999697000039\n",
      "Iteration 71: Error = 1.10004999983, a = 0.21100003799999723, b = 0.6839999758000044\n",
      "Iteration 72: Error = 1.10004999986, a = 0.2110000303999966, b = 0.6839999806000048\n",
      "Iteration 73: Error = 1.10004999989, a = 0.2110000242999961, b = 0.6839999845000051\n",
      "Iteration 74: Error = 1.10004999991, a = 0.2110000193999957, b = 0.6839999876000054\n",
      "Iteration 75: Error = 1.10004999993, a = 0.21100001549999536, b = 0.6839999901000056\n",
      "Iteration 76: Error = 1.10004999994, a = 0.2110000123999951, b = 0.6839999920000057\n",
      "Iteration 77: Error = 1.10004999996, a = 0.2110000099999949, b = 0.6839999936000059\n",
      "Iteration 78: Error = 1.10004999996, a = 0.21100000799999474, b = 0.683999994800006\n",
      "Iteration 79: Error = 1.10004999997, a = 0.2110000063999946, b = 0.683999995800006\n",
      "Iteration 80: Error = 1.10004999998, a = 0.2110000050999945, b = 0.6839999967000061\n",
      "Iteration 81: Error = 1.10004999998, a = 0.21100000409999442, b = 0.6839999973000062\n",
      "Iteration 82: Error = 1.10004999999, a = 0.21100000329999435, b = 0.6839999979000062\n",
      "Iteration 83: Error = 1.10004999999, a = 0.2110000026999943, b = 0.6839999983000062\n",
      "Iteration 84: Error = 1.10004999999, a = 0.21100000219999426, b = 0.6839999986000063\n",
      "Iteration 85: Error = 1.10004999999, a = 0.21100000169999422, b = 0.6839999989000063\n",
      "Iteration 86: Error = 1.10004999999, a = 0.2110000012999942, b = 0.6839999991000063\n",
      "Iteration 87: Error = 1.10005, a = 0.21100000109999417, b = 0.6839999993000063\n",
      "Iteration 88: Error = 1.10005, a = 0.21100000089999416, b = 0.6839999995000063\n",
      "Iteration 89: Error = 1.10005, a = 0.21100000079999415, b = 0.6839999996000063\n",
      "Iteration 90: Error = 1.10005, a = 0.21100000069999414, b = 0.6839999997000064\n",
      "Iteration 91: Error = 1.10005, a = 0.21100000059999413, b = 0.6839999998000064\n",
      "Iteration 92: Error = 1.10005, a = 0.21100000049999412, b = 0.6839999999000064\n",
      "Iteration 93: Error = 1.10005, a = 0.21100000039999411, b = 0.6840000000000064\n",
      "Iteration 94: Error = 1.10005, a = 0.21100000039999411, b = 0.6840000000000064\n",
      "Iteration 95: Error = 1.10005, a = 0.21100000039999411, b = 0.6840000000000064\n",
      "Iteration 96: Error = 1.10005, a = 0.21100000039999411, b = 0.6840000000000064\n",
      "Iteration 97: Error = 1.10005, a = 0.21100000039999411, b = 0.6840000000000064\n",
      "Iteration 98: Error = 1.10005, a = 0.21100000039999411, b = 0.6840000000000064\n",
      "Iteration 99: Error = 1.10005, a = 0.21100000039999411, b = 0.6840000000000064\n",
      "Iteration 100: Error = 1.10005, a = 0.21100000039999411, b = 0.6840000000000064\n",
      "Optimal values: a = 0.21100000039999411, b = 0.6840000000000064\n"
     ]
    }
   ],
   "source": [
    "# Run gradient descent\n",
    "optimal_a, optimal_b = gradient_descent()\n",
    "print(f\"Optimal values: a = {optimal_a}, b = {optimal_b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation of the Gradient\n",
    "In the absence of an explicit formula to compute the derivative, we estimate the gradient using a method called the finite difference method. Specifically, we use the `forward difference` approximation for the partial derivatives:\n",
    "1. **For `a`:**\n",
    "$$ \\frac{\\partial \\text{Error}}{\\partial a} = \\frac{\\text{Error}(a+\\delta, b) - \\text{Error}(a,b)}{\\delta} $$\n",
    "\n",
    "2. **For `b`:**\n",
    "$$ \\frac{\\partial \\text{Error}}{\\partial b} = \\frac{\\text{Error}(a, b+\\delta) - \\text{Error}(a,b)}{\\delta} $$\n",
    "\n",
    "Here, $ \\delta $ is a small positive number that helps approximate the slope of the error function at a given point `(a, b)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Choices:\n",
    "\n",
    "1. **Initial Values `a_start=0.5` and `b_start=0.5`:** These are the starting values for \\(a\\) and \\(b\\). Starting at the midpoint of the allowed parameter range seemed like a neutral choice, but depending on prior knowledge or other considerations, different starting points could be chosen.\n",
    "\n",
    "2. **Learning Rate `learning_rate=0.1`:** This determines the step size in the direction of the negative gradient. A smaller learning rate might converge more reliably but slower, whereas a larger learning rate might converge faster but risks overshooting the minimum.\n",
    "\n",
    "3. **Iterations `iterations=100`:** This is the number of times the algorithm will update the parameters. This choice means we are allowing the algorithm up to 100 updates to find the optimal parameters. This is an arbitrary choice and in practice, might be set based on when the changes in error or parameters become negligibly small.\n",
    "\n",
    "4. **Delta `delta=0.01`:** This small value is used to approximate the gradient. The choice of $ \\delta $ represents a trade-off: a smaller $ \\delta $ might give a more accurate approximation of the gradient but could be more susceptible to numerical errors, while a larger $ \\delta $ might be less accurate but more stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justifications:\n",
    "\n",
    "1. **Gradient Estimation:** The forward difference method provides a simple and intuitive way to estimate the gradient. It's essentially measuring the \"rise over run\" over a very short distance, which approximates the instantaneous rate of change.\n",
    "\n",
    "2. **Learning Rate:** The chosen value is a commonly used starting point in gradient descent. It's a middle-ground choice that's neither too small nor too large. However, in practice, this might be tuned based on the problem.\n",
    "\n",
    "3. **Iterations:** While 100 iterations is an arbitrary choice, it often suffices for many problems. In a more refined version, one might implement a convergence criterion, like if the difference in error between successive iterations is below a certain threshold.\n",
    "\n",
    "4. **Delta:** The value of 0.01 for $ \\delta $ is a typical choice for numerical differentiation in the unit interval [0, 1]. It's a balance between accuracy and stability. Too small a $ \\delta $ could lead to numerical instability, while too large a $ \\delta $ could lead to inaccurate gradient estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find both the local and global minima, we can use the gradient descent method, as previously discussed. However, we need to address the challenge of the presence of multiple minima in our error surface. \n",
    "\n",
    "1. **Multiple Starting Points**: To find both local and global minima, we will run the gradient descent algorithm from different initial values of `(a, b)`. The idea is that, depending on our starting point, we may converge to different minima.\n",
    "\n",
    "2. **Determining Local vs. Global Minima**: Once we have the minima locations, we can compare the error values at these points. The one with the lowest error is the global minimum, and the other is the local minimum.\n",
    "\n",
    "3. **Validation for Local vs. Global Minima**: If we did not know how many minima were present, we would have used techniques like:\n",
    "    - **Grid Search**: A systematic search through a subset of the parameter space, while not exhaustive, can give an idea of regions of interest that may contain minima.\n",
    "    - **Random Restart**: We would run gradient descent multiple times with random initial values for `(a, b)`, and note the different minima we arrive at. If we keep arriving at the same minimum, it’s likely global, but if we find multiple, it indicates the presence of multiple minima.\n",
    "    - **Higher Order Derivatives**: A second-order derivative or the Hessian can be useful. A positive value indicates a local minimum, while a negative value indicates a local maximum. However, computing this for complex functions can be challenging.\n",
    "\n",
    "I'll now query the API from multiple starting points to find both the local and global minima. Let's implement this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Minimum: (0.7070000332000018, 0.16399998909999428)\n",
      "Local Minimum: (0.21099993040000423, 0.683999633400003)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "# Define the error function based on API query\n",
    "def get_error(a, b):\n",
    "    return float(requests.get(f\"http://ramcdougal.com/cgi-bin/error_function.py?a={a}&b={b}\", \n",
    "                              headers={\"User-Agent\": \"GradientDescentMinFinder\"}).text)\n",
    "\n",
    "# Define the gradient estimation\n",
    "def gradient(a, b, delta=0.01):\n",
    "    dE_da = (get_error(a + delta, b) - get_error(a, b)) / delta\n",
    "    dE_db = (get_error(a, b + delta) - get_error(a, b)) / delta\n",
    "    return dE_da, dE_db\n",
    "\n",
    "# Gradient Descent\n",
    "def gradient_descent(a_start, b_start, learning_rate=0.1, max_iters=100, tolerance=1e-6):\n",
    "    a, b = a_start, b_start\n",
    "    for i in range(max_iters):\n",
    "        dE_da, dE_db = gradient(a, b)\n",
    "        a -= learning_rate * dE_da\n",
    "        b -= learning_rate * dE_db\n",
    "        \n",
    "        # Stopping criteria\n",
    "        if np.sqrt(dE_da**2 + dE_db**2) < tolerance:\n",
    "            break\n",
    "    return a, b\n",
    "\n",
    "# Using multiple starting points to find minima\n",
    "starting_points = [(0.1, 0.1), (0.9, 0.9), (0.1, 0.9), (0.9, 0.1)]\n",
    "minima = [gradient_descent(a, b) for a, b in starting_points]\n",
    "\n",
    "# Checking error at found minima to determine global vs. local\n",
    "errors = [get_error(a, b) for a, b in minima]\n",
    "global_minimum = minima[np.argmin(errors)]\n",
    "local_minimum = minima[np.argmax(errors)]\n",
    "\n",
    "print(f\"Global Minimum: {global_minimum}\")\n",
    "print(f\"Local Minimum: {local_minimum}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
